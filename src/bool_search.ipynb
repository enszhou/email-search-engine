{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import email\n",
    "import time\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "punct_tran_table = str.maketrans(string.punctuation, \" \"*len(string.punctuation))\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "pos_tran = {\n",
    "    \"N\": wordnet.NOUN,\n",
    "    \"V\": wordnet.VERB,\n",
    "    \"J\": wordnet.ADJ,\n",
    "    \"R\": wordnet.ADV\n",
    "}\n",
    "\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "lancaster_stemmer = nltk.stem.lancaster.LancasterStemmer()\n",
    "snowball_stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "stemmer = snowball_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2str(doc_fp):\n",
    "    try:\n",
    "        mail = email.parser.Parser().parse(doc_fp)\n",
    "        mail_subject = mail.get('Subject')\n",
    "        mail_body = mail.get_payload()\n",
    "        return mail_subject + ' ' + mail_body\n",
    "    except Exception as e:\n",
    "        return \"the\"\n",
    "\n",
    "\n",
    "def tokenize(doc_str):\n",
    "    doc_str = doc_str.translate(punct_tran_table)\n",
    "    tokens = nltk.tokenize.word_tokenize(doc_str)\n",
    "    return tokens\n",
    "\n",
    "def lem(tokens):\n",
    "    token_tags = nltk.pos_tag(tokens)\n",
    "    stem_tokens = []\n",
    "    for token, tag in token_tags:\n",
    "        token = token.lower()\n",
    "        if tag[0] in pos_tran:\n",
    "            stem_tokens.append(lemmer.lemmatize(token, pos=pos_tran[tag[0]]))\n",
    "        else:\n",
    "            stem_tokens.append(token)\n",
    "    return stem_tokens\n",
    "\n",
    "\n",
    "def stem_single(toekn):\n",
    "    token = token.lower()\n",
    "    return stemmer.stem(token)\n",
    "\n",
    "\n",
    "def stem(tokens):\n",
    "    stem_tokens = []\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        stem_tokens.append(stemmer.stem(token))\n",
    "    return stem_tokens\n",
    "\n",
    "\n",
    "def del_duplicates(tokens):\n",
    "    # remove duplicates in tokens\n",
    "    tokens = list(set(tokens))\n",
    "    return tokens\n",
    "\n",
    "def del_stops(tokens):\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "def append_tokens(tokens, doc_id, inverted_indices):\n",
    "    for token in tokens:\n",
    "        if token in inverted_indices:\n",
    "            if doc_id != inverted_indices[token][-1]:\n",
    "                inverted_indices[token].append(doc_id)\n",
    "        else:\n",
    "            inverted_indices[token] = [doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate the mapping file between mail ids and paths\n",
    "dataset_path = \"../dataset/\"\n",
    "id_path_map = \"../output/id_path_map.txt\"\n",
    "\n",
    "with open(id_path_map,\"w+\") as f_id_path_map:\n",
    "    num_files = 0\n",
    "    for root, dirs, files in os.walk(dataset_path, topdown=False):\n",
    "        for f in files:\n",
    "            f_id_path_map.write(str(num_files)+' '+os.path.join(root.replace(dataset_path, \"\"),f) +'\\n')\n",
    "            num_files += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "id_path_map = \"../output/id_path_map_part.txt\"\n",
    "inverted_indices = {}\n",
    "cost_time = [0,0,0,0,0,0]\n",
    "temp_time = [0,0,0,0,0,0]\n",
    "with open(id_path_map,\"r\") as f_id_path_map:\n",
    "    while True:\n",
    "        line = f_id_path_map.readline().strip('\\n')\n",
    "        if not line:\n",
    "            break\n",
    "        doc_id, doc_path = line.split(\" \")\n",
    "        doc_id = int(doc_id)\n",
    "        # read docs\n",
    "        with open(os.path.join(\"..\",\"dataset\",doc_path),\"r\") as doc_fp:\n",
    "            temp_time[0] = time.time()\n",
    "            doc_str = doc2str(doc_fp)\n",
    "            temp_time[1] = time.time()\n",
    "            tokens = tokenize(doc_str)\n",
    "            temp_time[2] = time.time()\n",
    "            # tokens = stem(tokens)\n",
    "            with Pool(4) as p:\n",
    "                tokens = p.map(stem_single, tokens)\n",
    "            temp_time[3] = time.time()\n",
    "            tokens = del_stops(tokens)\n",
    "            temp_time[4] = time.time()\n",
    "            # add tokens of a certain doc into inverted index table\n",
    "            append_tokens(tokens, doc_id, inverted_indices)    \n",
    "            temp_time[5] = time.time()\n",
    "            for i in range(5):\n",
    "                cost_time[i] += temp_time[i+1] - temp_time[i]\n",
    "            cost_time[5] = sum(cost_time[:-2])\n",
    "cost_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../dataset/'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "os.path.join(\"..\", \"dataset\",\"\")"
   ]
  }
 ]
}