{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "dd8909e800e94bf04c8fda69fdf8d84b95ba4a77e84d966b2a1e109c79a1a0ec"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import email\n",
    "import time\n",
    "import string\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "punct_tran_table = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "pos_tran = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"J\": wordnet.ADJ, \"R\": wordnet.ADV}\n",
    "\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "lancaster_stemmer = nltk.stem.lancaster.LancasterStemmer()\n",
    "snowball_stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "stemmer = snowball_stemmer\n",
    "\n",
    "\n",
    "def gen_id_path_map(dataset_path, id_path_map):\n",
    "    with open(id_path_map, \"w+\") as f_id_path_map:\n",
    "        num_files = 0\n",
    "        for root, dirs, files in os.walk(dataset_path, topdown=False):\n",
    "            for f in files:\n",
    "                f_id_path_map.write(\n",
    "                    str(num_files)\n",
    "                    + \" \"\n",
    "                    + os.path.join(root.replace(dataset_path, \"\"), f)\n",
    "                    + \"\\n\"\n",
    "                )\n",
    "                num_files += 1\n",
    "    return num_files\n",
    "\n",
    "\n",
    "def doc2str(doc_fp):\n",
    "    try:\n",
    "        mail = email.parser.Parser().parse(doc_fp)\n",
    "        mail_subject = mail.get(\"Subject\")\n",
    "        mail_body = mail.get_payload()\n",
    "        return mail_subject + \" \" + mail_body\n",
    "    except Exception as e:\n",
    "        return \"the\"\n",
    "\n",
    "\n",
    "def tokenize(doc_str):\n",
    "    doc_str = doc_str.translate(punct_tran_table)\n",
    "    tokens = nltk.tokenize.word_tokenize(doc_str)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def lem(tokens):\n",
    "    token_tags = nltk.pos_tag(tokens)\n",
    "    stem_tokens = []\n",
    "    for token, tag in token_tags:\n",
    "        token = token.lower()\n",
    "        if tag[0] in pos_tran:\n",
    "            stem_tokens.append(lemmer.lemmatize(token, pos=pos_tran[tag[0]]))\n",
    "        else:\n",
    "            stem_tokens.append(token)\n",
    "    return stem_tokens\n",
    "\n",
    "\n",
    "def stem(token):\n",
    "    token = token.lower()\n",
    "    return stemmer.stem(token)\n",
    "\n",
    "\n",
    "def del_duplicates(tokens):\n",
    "    # remove duplicates in tokens\n",
    "    tokens = list(set(tokens))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def del_stop(token):\n",
    "    return token not in stopwords\n",
    "\n",
    "\n",
    "def append_tokens(tokens, doc_id, inverted_indices):\n",
    "    for token in tokens:\n",
    "        if token in inverted_indices:\n",
    "            if doc_id != inverted_indices[token][-1]:\n",
    "                inverted_indices[token].append(doc_id)\n",
    "        else:\n",
    "            inverted_indices[token] = [doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "[0.19599294662475586, 0.749335527420044, 0.0019948482513427734, 0.0009961128234863281, 1.7145347595214844, 2.6628541946411133]\n"
     ]
    }
   ],
   "source": [
    "# run only once to generate map file between doc id and doc path\n",
    "dataset_path = os.path.join(\"..\", \"dataset\", \"\")\n",
    "id_path_map = os.path.join(\"..\", \"output\", \"id_path_map.txt\")\n",
    "# gen_id_path_map(dataset_path, id_path_map)\n",
    "\n",
    "max_iters = 1000\n",
    "inverted_indices = {}\n",
    "cost_time = [0, 0, 0, 0, 0, 0]\n",
    "temp_time = [0, 0, 0, 0, 0, 0]\n",
    "with open(id_path_map, \"r\") as f_id_path_map:\n",
    "    iter = 0\n",
    "    while True:\n",
    "        if iter % 100 == 0:\n",
    "            print(iter)\n",
    "        iter += 1\n",
    "        line = f_id_path_map.readline().strip(\"\\n\")\n",
    "        if not line or iter > max_iters:\n",
    "            break\n",
    "        doc_id, doc_path = line.split(\" \")\n",
    "        doc_id = int(doc_id)\n",
    "        # read docs\n",
    "        with open(os.path.join(\"..\", \"dataset\", doc_path), \"r\") as doc_fp:\n",
    "            temp_time[0] = time.time()\n",
    "            doc_str = doc2str(doc_fp)\n",
    "            temp_time[1] = time.time()\n",
    "            tokens = tokenize(doc_str)\n",
    "            temp_time[2] = time.time()\n",
    "            tokens = map(stem, tokens)\n",
    "            temp_time[3] = time.time()\n",
    "            tokens = filter(del_stop, tokens)\n",
    "            temp_time[4] = time.time()\n",
    "            # add tokens of a certain doc into inverted index table\n",
    "            append_tokens(tokens, doc_id, inverted_indices)\n",
    "            temp_time[5] = time.time()\n",
    "            for i in range(5):\n",
    "                cost_time[i] += temp_time[i + 1] - temp_time[i]\n",
    "            cost_time[5] = sum(cost_time[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  }
 ]
}