{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "dd8909e800e94bf04c8fda69fdf8d84b95ba4a77e84d966b2a1e109c79a1a0ec"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate total terms frequency of top 1000\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import email\n",
    "import time\n",
    "import string\n",
    "from collections import Counter\n",
    "import yaml\n",
    "\n",
    "\n",
    "punct_tran_table = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "pos_tran = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"J\": wordnet.ADJ, \"R\": wordnet.ADV}\n",
    "\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "lancaster_stemmer = nltk.stem.lancaster.LancasterStemmer()\n",
    "snowball_stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "stemmer = snowball_stemmer\n",
    "\n",
    "\n",
    "def gen_id_path_map(dataset_path, id_path_map):\n",
    "    with open(id_path_map, \"w+\") as f_id_path_map:\n",
    "        num_files = 0\n",
    "        for root, dirs, files in os.walk(dataset_path, topdown=False):\n",
    "            for f in files:\n",
    "                f_id_path_map.write(\n",
    "                    str(num_files)\n",
    "                    + \" \"\n",
    "                    + os.path.join(root.replace(dataset_path, \"\"), f)\n",
    "                    + \"\\n\"\n",
    "                )\n",
    "                num_files += 1\n",
    "    return num_files\n",
    "\n",
    "\n",
    "def doc2str(doc_fp):\n",
    "    try:\n",
    "        mail = email.parser.Parser().parse(doc_fp)\n",
    "        mail_subject = mail.get(\"Subject\")\n",
    "        mail_body = mail.get_payload()\n",
    "        return mail_subject + \" \" + mail_body\n",
    "    except Exception as e:\n",
    "        return \"the\"\n",
    "\n",
    "\n",
    "def tokenize(doc_str):\n",
    "    doc_str = doc_str.translate(punct_tran_table)\n",
    "    tokens = nltk.tokenize.word_tokenize(doc_str)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def lem(tokens):\n",
    "    token_tags = nltk.pos_tag(tokens)\n",
    "    stem_tokens = []\n",
    "    for token, tag in token_tags:\n",
    "        token = token.lower()\n",
    "        if tag[0] in pos_tran:\n",
    "            stem_tokens.append(lemmer.lemmatize(token, pos=pos_tran[tag[0]]))\n",
    "        else:\n",
    "            stem_tokens.append(token)\n",
    "    return stem_tokens\n",
    "\n",
    "\n",
    "def stem(token):\n",
    "    token = token.lower()\n",
    "    return stemmer.stem(token)\n",
    "\n",
    "\n",
    "\n",
    "def del_stop(token):\n",
    "    return token not in stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run only once to generate map file between doc id and doc path\n",
    "dataset_path = os.path.join(\"..\", \"dataset\", \"\")\n",
    "id_path_map = os.path.join(\"..\", \"output\", \"id_path_map.txt\")\n",
    "# gen_id_path_map(dataset_path, id_path_map)\n",
    "\n",
    "max_iters = 1000000\n",
    "all_tokens = []\n",
    "total_tf_table = {}\n",
    "cost_time = [0, 0, 0, 0, 0, 0]\n",
    "temp_time = [0, 0, 0, 0, 0, 0]\n",
    "with open(id_path_map, \"r\") as f_id_path_map:\n",
    "    iter = 0\n",
    "    while True:\n",
    "        if iter % 1000 == 0:\n",
    "            print(iter)\n",
    "        iter += 1\n",
    "        line = f_id_path_map.readline().strip(\"\\n\")\n",
    "        if not line or iter > max_iters:\n",
    "            break\n",
    "        doc_id, doc_path = line.split(\" \")\n",
    "        doc_id = int(doc_id)\n",
    "        # read docs\n",
    "        with open(os.path.join(\"..\", \"dataset\", doc_path), \"r\") as doc_fp:\n",
    "            temp_time[0] = time.time()\n",
    "            doc_str = doc2str(doc_fp)\n",
    "            temp_time[1] = time.time()\n",
    "            tokens = tokenize(doc_str)\n",
    "            temp_time[2] = time.time()\n",
    "            tokens = map(stem, tokens)\n",
    "            temp_time[3] = time.time()\n",
    "            tokens = filter(del_stop, tokens)\n",
    "            temp_time[4] = time.time()\n",
    "            all_tokens.extend(tokens)\n",
    "            temp_time[5] = time.time()\n",
    "            for i in range(5):\n",
    "                cost_time[i] += temp_time[i + 1] - temp_time[i]\n",
    "            cost_time[5] = sum(cost_time[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tf_table = Counter(all_tokens)\n",
    "with open(\"../output/total_tf.yaml\",\"w+\") as fp:\n",
    "    yaml.dump(total_tf_table, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_tf_table = total_tf_table.most_common()\n",
    "with open(\"../output/total_tf_ordered.yaml\",\"w+\") as fp:\n",
    "    yaml.dump(ordered_tf_table, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_table_1000 = total_tf_table.most_common(1000)\n",
    "with open(\"../output/total_tf_1000.yaml\",\"w+\") as fp:\n",
    "    yaml.dump(tf_table_1000, fp)"
   ]
  }
 ]
}